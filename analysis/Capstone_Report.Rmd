---
title: "Capstone Report"
author: "William Matthews"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    css: custom.css
---

Report generated on: `r format(Sys.time(), "%B %d, %Y at %H:%M:%S")`

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(usmap)
library(scales)
library(tibble)
library(stringr)
library(purrr)
library(gridExtra)
library(zoo)
library(lubridate)
library(tidyr)

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

```{r create-output-folder, include=FALSE}

# New structure:
output_dir <- "../visualizations"

# Old structure:
# output_dir <- "G:/Capstone/Capstone Visuals"

if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}
```
## Project Overview & Agenda

**Objective:** Analyze credit card fraud patterns to identify actionable risk mitigation strategies

**Analysis Framework:**
- Geographic Risk Assessment: Examine fraud rate variations across states to identify high-risk regions 
  state with 1.16% fraud rate, confirmed through both observed data and Bayesian analysis.
- Merchant-level fraud patterns and risk concentration  
- Temporal trend analysis (daily, weekly, monthly patterns)
- Statistical validation using methods appropriate to data characteristics and sample sizes
- Evidence-based recommendations with implementation guidance

**Key Questions We'll Answer:**
- Which geographic regions pose the highest fraud risk?
- Which merchants show elevated fraud patterns?
- What temporal patterns can guide monitoring strategies?
- How can we make statistically sound, data-driven decisions?

```
### Data Loading and Initial Preparation

**What:**         Loads the credit card fraud dataset from a CSV file and maps transaction cities to their corresponding                    states using a static lookup table.

**Why:**          To enrich the dataset with geographic information at the state level, which is essential for analyzing                    the spatial distribution of fraud cases.

**Purpose:**      To enable detailed geographic fraud analysis and help identify high-risk regions for targeted                             interventions.

**Limitations:**  The city-to-state mapping relies on a manually curated list of cities and may not cover all locations                     present in the data, potentially leaving some transactions unmapped. Additionally, the analysis assumes                   the source data is accurate and consistent.

How we do it:
We load the original credit card fraud dataset to preserve readable merchant names, then separately load an enriched dataset containing city-to-state mappings. After creating a city column from the location data, we perform a left join to add state information to each transaction record. We immediately validate the geographic mapping success rate and flag any data quality issues that could affect subsequent geographic analysis reliability.
```{r}

```{r load-and-prepare-data}

# Load original dataset (has readable merchant names)
#New Structure:
creditCardFraud <- read_csv("../data/credit_card_fraud_dataset.csv")

#Old Structure
#creditCardFraud <- read_csv("G:/Capstone/Data/credit_card_fraud_dataset.csv")


enriched_geo <- read_csv("../data/credit_card_fraud_dataset_enriched.csv")

# Load enriched dataset just to extract the city-state mapping
#enriched_geo <- read_csv("G:/Capstone/Data/credit_card_fraud_dataset_enriched.csv") %>%
#  dplyr::select(City, State) %>%
#  dplyr::distinct()

# Join the geographic data to the original dataset
creditCardFraud <- creditCardFraud %>%
  dplyr::mutate(City = Location) %>%
  dplyr::left_join(enriched_geo, by = "City")

# Validate geographic mapping success
mapping_stats <- creditCardFraud %>%
  summarise(
    total_records = n(),
    mapped_records = sum(!is.na(State)),
    mapping_success_rate = round(100 * mapped_records / total_records, 1),
    unique_cities_total = n_distinct(City),
    unique_cities_mapped = n_distinct(City[!is.na(State)])
  )

cat("=== GEOGRAPHIC MAPPING VALIDATION ===\n")
cat("Total records:", mapping_stats$total_records, "\n")
cat("Successfully mapped:", mapping_stats$mapped_records, 
    "(", mapping_stats$mapping_success_rate, "%)\n")
cat("Unique cities total:", mapping_stats$unique_cities_total, "\n")
cat("Unique cities mapped:", mapping_stats$unique_cities_mapped, "\n")

# Flag potential data quality issues
if (mapping_stats$mapping_success_rate < 95) {
  cat("WARNING: Geographic mapping success rate below 95%\n")
}


```
### Summarizing Fraud Cases by State

**What:**         Filters fraud cases and counts the total number by state.  
**Why:**          To identify geographic hotspots of fraud activity.  
**Purpose:**      Help prioritize investigation and mitigation resources by state.  
**Limitations:**  Records with missing or invalid state information are excluded, which may affect completeness of the                      geographic analysis.

How we do it:
We group all transactions by state and calculate three key metrics for each: total transaction count, fraud case count, and fraud rate percentage. We filter out states with fewer than 20 total cases to ensure statistical reliability, then sort results by descending fraud rate to identify the highest-risk geographic regions for targeted fraud prevention efforts.

```{r}
fraud_by_state <- creditCardFraud %>%
  group_by(State) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud),
    FraudRate = 100 * FraudCases / TotalCases,
    .groups = "drop"
  ) %>%
  filter(!is.na(State), TotalCases >= 20) %>%
  arrange(desc(FraudRate))

print(fraud_by_state)

# Display key metrics for stakeholders
cat("\n=== KEY FINDINGS ===\n")
cat("Highest fraud rate state:", fraud_by_state$State[1], 
    "with rate:", round(fraud_by_state$FraudRate[1], 1), "%\n")
cat("Total states analyzed:", nrow(fraud_by_state), "\n")
```

### Identifying the Top Fraud State and Merchant

**What:**         Identifies the state and merchant with the highest fraud counts.  
**Why:**          To focus efforts on the most impacted geographic area and merchant.  
**Purpose:**      Prioritize investigation and resource allocation effectively.  
**Limitations:**  Dependent on data accuracy and completeness, including city-to-state mapping reliability.

How we do it:
We identify the state with the highest fraud rate from our geographic analysis, then filter the dataset to focus on merchants operating within that state. For each merchant, we calculate total transactions, fraud cases, and fraud rates, applying a minimum sample size filter of 10 transactions to ensure statistical validity. We then select the merchant with the highest fraud rate for detailed analysis.


```{r}
top_state <- fraud_by_state %>%
  slice_max(FraudRate, n = 1) %>%
  pull(State)

# Find merchant with highest fraud rate in top state
top_merchant_analysis <- creditCardFraud %>%
  filter(State == top_state) %>%
  filter(!is.na(MerchantName), str_trim(MerchantName) != "") %>%
  group_by(MerchantName) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud),
    FraudRate = 100 * FraudCases / TotalCases,
    .groups = "drop"
  ) %>%
  filter(TotalCases >= 10) %>%
  arrange(desc(FraudRate))

top_merchant <- top_merchant_analysis %>%
  slice_max(FraudRate, n = 1) %>%
  pull(MerchantName)

cat("Top fraud state:", top_state, "\n")
cat("Top merchant in", top_state, ":", top_merchant, "\n")
cat("Merchant fraud rate:", round(top_merchant_analysis$FraudRate[1], 1), "%\n")

```

### Defining Coordinates for State Labels on the Map

**What:**         Defines approximate geographic coordinates for key states to position labels on the fraud map.  
**Why:**          To accurately place dynamic labels on the visualization.  
**Purpose:**      Enhance map readability and clarity for stakeholders.  
**Limitations:**  Coordinates are approximate and may require adjustment if states are added or label positions change.

How we do it:
We identify the state with the highest fraud rate from our geographic analysis, then filter the dataset to focus on merchants operating within that state. For each merchant, we calculate total transactions, fraud cases, and fraud rates, applying a minimum sample size filter of 10 transactions to ensure statistical validity. We then select the merchant with the highest fraud rate for detailed analysis.

```{r}
state_coords <- tibble::tribble(
  ~state, ~x,     ~y,
  "AL",   -86.8,  32.8,
  "AK",   -152.0, 64.0,
  "AZ",   -112.0, 34.0,
  "AR",   -92.2,  34.8,
  "CA",   -120.0, 37.0,
  "CO",   -105.5, 39.0,
  "CT",   -72.7,  41.6,
  "DE",   -75.5,  39.0,
  "FL",   -81.5,  28.0,
  "GA",   -83.5,  32.5,
  "HI",   -157.8, 21.3,
  "ID",   -114.0, 44.0,
  "IL",   -89.0,  40.0,
  "IN",   -86.0,  39.5,
  "IA",   -93.5,  42.0,
  "KS",   -98.0,  38.5,
  "KY",   -85.0,  37.5,
  "LA",   -92.0,  31.0,
  "ME",   -69.5,  45.0,
  "MD",   -76.5,  39.0,
  "MA",   -71.5,  42.3,
  "MI",   -85.0,  44.5,
  "MN",   -94.0,  46.0,
  "MS",   -89.5,  32.5,
  "MO",   -92.5,  38.5,
  "MT",   -110.0, 47.0,
  "NE",   -99.5,  41.5,
  "NV",   -117.0, 39.0,
  "NH",   -71.5,  44.0,
  "NJ",   -74.5,  40.0,
  "NM",   -106.0, 34.5,
  "NY",   -74.0,  43.0,
  "NC",   -79.0,  35.5,
  "ND",   -100.0, 47.5,
  "OH",   -82.5,  40.0,
  "OK",   -97.0,  35.5,
  "OR",   -120.5, 44.0,
  "PA",   -77.0,  41.0,
  "RI",   -71.5,  41.7,
  "SC",   -81.0,  34.0,
  "SD",   -100.0, 44.5,
  "TN",   -86.0,  36.0,
  "TX",   -100.0, 31.0,
  "UT",   -111.5, 39.5,
  "VT",   -72.5,  44.0,
  "VA",   -78.0,  37.5,
  "WA",   -120.5, 47.5,
  "WV",   -80.5,  38.5,
  "WI",   -89.5,  44.5,
  "WY",   -107.5, 43.0
)

label_coords <- state_coords %>%
  filter(state == top_state) %>%
  select(x, y) %>%
  as.list()
```

### Visualizing Fraud Cases by State with Dynamic Label

**What:**         Creates a heatmap showing fraud case counts by state, highlighting the top fraud state with a label.  
**Why:**          To visually communicate geographic distribution of fraud cases.  
**Purpose:**      Help stakeholders quickly identify high-risk states for targeted action.  
**Limitations:**  Color gradients may obscure exact counts; label positioning is approximate and may require fine-tuning.

How we do it:
We prepare the fraud data by renaming columns to match mapping package requirements, then create a choropleth map using the usmap package with fraud rates determining color intensity. We add dynamic annotations using the coordinate lookup system, placing labels and arrows to highlight the highest fraud rate state. The completed visualization is saved as a high-resolution image for reporting purposes.



```{r}
plot_data <- fraud_by_state %>%
  rename(state = State)

max_rate <- max(plot_data$FraudRate, na.rm = TRUE)

base_map <- plot_usmap(data = plot_data, values = "FraudRate", color = "black") +
  scale_fill_continuous(
    low = "lightyellow", high = "red", name = "Fraud Rate (%)",
    label = scales::comma,
    limits = c(0, max_rate)
  ) +
  labs(
    title = "Credit Card Fraud Rate by State",
    subtitle = paste("Highest risk state:", top_state, "at", 
                     round(fraud_by_state$FraudRate[1], 1), "%"),
    caption = "Source: Credit Card Fraud Dataset"
  ) +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.position = "right",
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 11, face = "bold"),
    plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
  )

# Create the final map with annotation
final_map <- base_map +
  annotate(
    "label",
    x = label_coords$x,
    y = label_coords$y,
    label = paste(top_state, ": Highest Fraud"),
    size = 5,
    fontface = "bold",
    fill = "white",
    color = "black",
    label.size = 0.5
  ) +
  annotate(
    "segment",
    x = label_coords$x,
    xend = label_coords$x + 2,
    y = label_coords$y,
    yend = label_coords$y - 2,
    colour = "black",
    size = 1,
    arrow = arrow(length = unit(0.3, "cm"))
  )

print(final_map)

ggsave(paste0(output_dir, "/fraud_cases_by_state.png"), 
       plot = final_map, width = 8, height = 6, dpi = 300)

```
### Drilling Down into High-Risk Geographic Areas

Having identified the highest-risk state, we now examine which specific merchants within this region are driving the elevated fraud rate.


### Identifying Invalid or Placeholder Merchant Names

**What:**         Detects missing or placeholder merchant names such as "NA", "unknown", or blank entries.  
**Why:**          To identify data quality issues that could affect merchant-level fraud analysis accuracy.  
**Purpose:**      Ensure reliability of analyses and visualizations involving merchant names.  
**Limitations:**  The list of placeholders may not be exhaustive; some invalid names might not be detected.

How we do it:
We define a validation function that flags merchant names as invalid if they are missing, blank, or match common placeholder patterns like "NA" or "unknown." We then analyze fraud cases within the top risk state to quantify data quality issues, calculating the percentage of cases with invalid merchant names and providing examples of problematic entries when they exist.


```{r}

is_invalid_name <- function(x) {
  is.na(x) | str_trim(x) == "" | tolower(str_trim(x)) %in% c("na", "n/a", "unknown", "#n/a")
}

# Check for invalid merchant names in top state fraud cases
invalid_names_check <- creditCardFraud %>%
  filter(State == top_state, IsFraud == 1) %>%
  summarise(
    total_fraud_cases = n(),
    invalid_names = sum(is_invalid_name(MerchantName)),
    pct_invalid = round(100 * invalid_names / total_fraud_cases, 1)
  )

cat("=== MERCHANT NAME DATA QUALITY ===\n")
cat("Total fraud cases in", top_state, ":", invalid_names_check$total_fraud_cases, "\n")
cat("Cases with invalid merchant names:", invalid_names_check$invalid_names, 
    "(", invalid_names_check$pct_invalid, "%)\n")

# Show examples of invalid names if any exist
if (invalid_names_check$invalid_names > 0) {
  cat("\nExamples of invalid merchant names:\n")
  creditCardFraud %>%
    filter(State == top_state, IsFraud == 1) %>%
    filter(is_invalid_name(MerchantName)) %>%
    distinct(MerchantName) %>%
    slice_head(n = 5) %>%
    pull(MerchantName) %>%
    paste(collapse = ", ") %>%
    cat()
  cat("\n")
}

```
### Calculating Fraud Cases by Merchant Across All States

**What:**         Aggregates fraud case counts for each merchant across all states.  
**Why:**          To identify merchants with the highest overall fraud incidence.  
**Purpose:**      Prioritize fraud mitigation efforts at the merchant level.  
**Limitations:**  Includes all merchants regardless of name validity, which may introduce noise in the counts.

How we do it:
We filter the complete dataset to include only confirmed fraud cases, exclude merchants with invalid names, then group transactions by merchant to count total fraud cases per merchant. The results are sorted by fraud count in descending order to identify which merchants generate the highest absolute fraud volumes nationally, providing insights into fraud concentration patterns across the entire merchant network.


```{r}
merchant_fraud_analysis <- creditCardFraud %>%
  filter(IsFraud == 1) %>%
  filter(!is_invalid_name(MerchantName)) %>%
  group_by(MerchantName) %>%
  summarise(
    FraudCases = n(),
    .groups = "drop"
  ) %>%
  arrange(desc(FraudCases))

cat("=== NATIONAL MERCHANT FRAUD ANALYSIS ===\n")
cat("Total merchants with fraud cases:", nrow(merchant_fraud_analysis), "\n")
cat("Top merchant by fraud count:", merchant_fraud_analysis$MerchantName[1], 
    "with", merchant_fraud_analysis$FraudCases[1], "cases\n")

# Show top 10 merchants by fraud count
cat("\nTop 10 merchants by fraud case count:\n")
print(head(merchant_fraud_analysis, 10))

```
### Summarizing Fraud Case Totals for Top Merchants

**What:**         Calculates the total number of fraud cases attributed to the top N merchants and compares it to the                       overall fraud count.  
**Why:**          To assess whether fraud is concentrated among a few merchants or dispersed more widely.  
**Purpose:**      Inform strategic decisions on resource allocation and risk prioritization.  
**Limitations:**  Results are sensitive to the choice of N (number of top merchants considered), which should be selected                   thoughtfully based on context.

How we do it:  
This section sums the fraud cases for the top N merchants—where N is a configurable parameter—and compares this total to the overall number of fraud cases across all merchants. This provides insight into the concentration of fraud and helps prioritize efforts effectively.



```{r top_merchants_plot, fig.width=11, fig.height=6}
# Calculate fraud concentration among top merchants nationally
top_n_merchants <- 10

top_merchants_national <- merchant_fraud_analysis %>%
  slice_head(n = top_n_merchants)

concentration_stats <- list(
  total_fraud_cases = sum(merchant_fraud_analysis$FraudCases),
  top_n_fraud_cases = sum(top_merchants_national$FraudCases),
  concentration_pct = round(100 * sum(top_merchants_national$FraudCases) / sum(merchant_fraud_analysis$FraudCases), 1)
)

cat("=== FRAUD CONCENTRATION ANALYSIS ===\n")
cat("Top", top_n_merchants, "merchants account for", concentration_stats$top_n_fraud_cases, 
    "of", concentration_stats$total_fraud_cases, "total fraud cases\n")
cat("Concentration rate:", concentration_stats$concentration_pct, "%\n")

# Top merchants in the highest fraud rate state by rate
top_state_merchants <- creditCardFraud %>%
  filter(State == top_state) %>%
  filter(!is_invalid_name(MerchantName)) %>%
  group_by(MerchantName) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud),
    FraudRate = 100 * FraudCases / TotalCases,
    .groups = "drop"
  ) %>%
  filter(TotalCases >= 10) %>%
  arrange(desc(FraudRate)) %>%
  slice_head(n = 10) %>%
  mutate(MerchantName = factor(MerchantName, levels = MerchantName))

# Create visualization
ggplot(top_state_merchants, aes(x = MerchantName, y = FraudRate, fill = FraudRate)) +
  geom_col() +
  scale_fill_gradient(low = "lightyellow", high = "red", name = "Fraud Rate (%)") + 
  geom_text(aes(label = paste0(round(FraudRate, 1), "%")), hjust = -0.1, size = 4) +
  coord_flip(clip = "off") +
  expand_limits(y = max(top_state_merchants$FraudRate) * 1.1) +
  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0, 0.15))) +
  labs(
    title = paste("Top 10 Merchants by Fraud Rate in", top_state),
    subtitle = "Minimum 10 transactions required for inclusion",
    x = "Merchant",
    y = "Fraud Rate (%)",
    caption = "Source: Credit Card Fraud Dataset"
  ) +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 11, face = "bold"),
    plot.margin = ggplot2::margin(t = 20, r = 80, b = 20, l = 20),
    plot.title.position = "plot"
  )

ggsave(paste0(output_dir, "/top_10_merchants_", tolower(top_state), ".png"), 
       width = 8, height = 5, dpi = 300)

```
### Understanding Temporal Fraud Patterns

Beyond geographic and merchant-specific patterns, timing plays a crucial role in fraud prevention strategy. Let's examine how fraud fluctuates over different time scales—daily volatility, weekly patterns, and monthly trends—to optimize our monitoring intensity and resource allocation throughout the year.


### Multi-Scale Temporal Fraud Analysis: Daily, Weekly, and Monthly Perspectives

**What:**         Analyzes fraud case counts across multiple time scales using daily data with smoothing, weekly                              aggregations, and monthly totals to reveal patterns at different temporal granularities.
**Why:**          Daily fraud data contains significant noise that can obscure underlying trends, while single time-scale                     analysis may miss important patterns visible at other intervals.
**Purpose:**      Provide comprehensive temporal intelligence for strategic resource allocation, operational planning, and                    pattern recognition across short-term volatility, medium-term cycles, and long-term trends.
**Limitations:**  Assumes transaction timestamps are accurate and consistently formatted. Rolling average smoothing may mask                   brief but significant fraud spikes.
How we do it:
This analysis creates three complementary views of fraud timing patterns. Daily counts show raw volatility with a 7-day rolling average overlay to highlight underlying trends. Weekly aggregations smooth short-term fluctuations while preserving medium-term patterns useful for operational scheduling. Monthly totals reveal seasonal patterns and long-term trends essential for strategic planning. Together, these three time scales provide stakeholders with tactical, operational, and strategic temporal insights for fraud prevention resource optimization.


```{r fraud-daily-monthly-trends, fig.width=12, fig.height=5}

# Prepare data: extract date (check if already done)
if (!"TransactionDateOnly" %in% colnames(creditCardFraud)) {
  creditCardFraud <- creditCardFraud %>%
    mutate(TransactionDateOnly = as.Date(parse_date_time(TransactionDate, orders = c("mdy HMS", "mdy HM", "mdy"))))
}

# Daily fraud counts with 7-day rolling average
daily_fraud <- creditCardFraud %>%
  filter(IsFraud == 1) %>%
  filter(!is.na(TransactionDateOnly)) %>%
  group_by(TransactionDateOnly) %>%
  summarise(FraudCases = n(), .groups = "drop") %>%
  arrange(TransactionDateOnly) %>%
  mutate(FraudCases_Smooth = zoo::rollmean(FraudCases, k = 7, fill = NA, align = "right"))

# Weekly fraud counts
weekly_fraud <- daily_fraud %>%
  mutate(WeekStart = floor_date(TransactionDateOnly, "week")) %>%
  group_by(WeekStart) %>%
  summarise(FraudCases = sum(FraudCases), .groups = "drop") %>%
  arrange(WeekStart)

# Monthly fraud counts
monthly_fraud <- daily_fraud %>%
  mutate(YearMonth = floor_date(TransactionDateOnly, "month")) %>%
  group_by(YearMonth) %>%
  summarise(FraudCases = sum(FraudCases), .groups = "drop") %>%
  arrange(YearMonth)

# Plot 1: Daily fraud with smoothing
p1 <- ggplot(daily_fraud, aes(x = TransactionDateOnly)) +
  geom_line(aes(y = FraudCases), color = "lightblue", alpha = 0.6) +
  geom_line(aes(y = FraudCases_Smooth), color = "steelblue", linewidth = 1) +
  labs(title = "7-day Rolling Average",
       x = "Date", y = "Number of Fraud Cases") +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
  )

# Plot 2: Weekly fraud counts
p2 <- ggplot(weekly_fraud, aes(x = WeekStart, y = FraudCases)) +
  geom_line(color = "darkorange", linewidth = 1) +
  labs(title = "Weekly Fraud Cases",
       x = "Week Starting", y = "Number of Fraud Cases") +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
  )

# Plot 3: Monthly fraud counts
p3 <- ggplot(monthly_fraud, aes(x = YearMonth, y = FraudCases)) +
  geom_line(color = "firebrick", linewidth = 1) +
  geom_point(color = "firebrick", size = 2) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  labs(title = "Monthly Fraud Cases",
       x = "Month", y = "Number of Fraud Cases") +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
  )

# Arrange plots side by side
grid.arrange(p1, p2, p3, ncol = 3)

# Save individual plots
ggsave(paste0(output_dir, "/daily_fraud_smoothed.png"), plot = p1, width = 5, height = 5, dpi = 300)
ggsave(paste0(output_dir, "/weekly_fraud_trend.png"), plot = p2, width = 5, height = 5, dpi = 300)
ggsave(paste0(output_dir, "/monthly_fraud_trend_2.png"), plot = p3, width = 5, height = 5, dpi = 300)


```


### Multi-Scale Fraud Trend Analysis: Daily, Weekly, and Monthly Views

**What:**         Analyzes fraud case counts across daily, weekly, and monthly time scales with smoothing.  
**Why:**          To reveal fraud trends at multiple granularities and reduce noise.  
**Purpose:**      Provide stakeholders with a comprehensive view of fraud dynamics for better decision-making.  
**Limitations:**  Assumes timestamps are accurate; smoothing may mask short spikes.

How we do it:  
This analysis builds on the previous daily and monthly fraud trends by incorporating a 7-day rolling average smoothing of daily counts to reduce noise and highlight underlying patterns. Additionally, fraud counts are aggregated weekly and monthly to offer medium- and long-term perspectives. The resulting visualizations—displaying smoothed daily data alongside weekly and monthly totals—provide stakeholders with a comprehensive, multi-scale view of fraud activity, supporting both tactical responses and strategic planning.


```{r fraud-daily-weekly-monthly-trends, fig.width=15, fig.height=5}

# Prepare data: extract date
creditCardFraud <- creditCardFraud %>%
  mutate(TransactionDateOnly = as.Date(parse_date_time(TransactionDate, orders = c("mdy HMS", "mdy HM", "mdy"))))

# Daily fraud counts
daily_fraud <- creditCardFraud %>%
  filter(IsFraud == 1) %>%
  group_by(TransactionDateOnly) %>%
  summarise(FraudCases = n()) %>%
  arrange(TransactionDateOnly) %>%
  mutate(
    # 7-day rolling average smoothing to reduce noise
    FraudCases_Smooth = zoo::rollmean(FraudCases, k = 7, fill = NA, align = "right")
  )

# Weekly fraud counts (start of week)
weekly_fraud <- daily_fraud %>%
  mutate(WeekStart = floor_date(TransactionDateOnly, "week")) %>%
  group_by(WeekStart) %>%
  summarise(FraudCases = sum(FraudCases)) %>%
  arrange(WeekStart)

# Monthly fraud counts
monthly_fraud <- daily_fraud %>%
  mutate(YearMonth = floor_date(TransactionDateOnly, "month")) %>%
  group_by(YearMonth) %>%
  summarise(FraudCases = sum(FraudCases)) %>%
  arrange(YearMonth)

# Plot 1: Daily fraud with smoothing
p1 <- ggplot(daily_fraud, aes(x = TransactionDateOnly)) +
  geom_line(aes(y = FraudCases), color = "lightblue", alpha = 0.6) +
  geom_line(aes(y = FraudCases_Smooth), color = "steelblue", size = 1) +
  labs(title = "7-day Rolling Average",
       x = "Date", y = "Number of Fraud Cases") +
  theme_light() +
  theme_light() +
theme(
  plot.title = element_text(size = 16, face = "bold"),
  axis.title = element_text(size = 12, face = "bold"),
  axis.text = element_text(size = 10),
  axis.text.x = element_text(angle = 45, hjust = 1),
  plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
)  

# Plot 2: Weekly fraud counts
p2 <- ggplot(weekly_fraud, aes(x = WeekStart, y = FraudCases)) +
  geom_line(color = "darkorange", size = 1) +
  labs(title = "Weekly Fraud Cases",
       x = "Week Starting", y = "Number of Fraud Cases") +
  theme_light() +
  theme_light() +
theme(
  plot.title = element_text(size = 16, face = "bold"),
  axis.title = element_text(size = 12, face = "bold"),
  axis.text = element_text(size = 10),
  axis.text.x = element_text(angle = 45, hjust = 1),
  plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
)

# Plot 3: Monthly fraud counts
p3 <- ggplot(monthly_fraud, aes(x = YearMonth, y = FraudCases)) +
  geom_line(color = "firebrick", size = 1) +
  geom_point(color = "firebrick", size = 2) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  labs(title = "Monthly Fraud Cases",
       x = "Month", y = "Number of Fraud Cases") +
  theme_light() +
  theme_light() +
theme(
  plot.title = element_text(size = 16, face = "bold"),
  axis.title = element_text(size = 12, face = "bold"),
  axis.text = element_text(size = 10),
  axis.text.x = element_text(angle = 45, hjust = 1),
  plot.margin = ggplot2::margin(t = 20, r = 20, b = 20, l = 20)
)

# Arrange plots side by side
grid.arrange(p1, p2, p3, ncol = 3)

ggsave(paste0(output_dir, "/daily_fraud_smoothed.png"), plot = p1, width = 5, height = 5, dpi = 300)
ggsave(paste0(output_dir, "/weekly_fraud_trend.png"), plot = p2, width = 5, height = 5, dpi = 300)
ggsave(paste0(output_dir, "/monthly_fraud_trend_2"), plot = p3, width = 5, height = 5, dpi = 300)
                     
                     
#ggsave("G:/Capstone/Capstone Visuals/daily_fraud_smoothed.png", plot = p1, width = 5, height = 5, dpi = 300)  
#ggsave("G:/Capstone/Capstone Visuals/weekly_fraud_trend.png", plot = p2, width = 5, height = 5, dpi = 300)  
#ggsave("G:/Capstone/Capstone Visuals/monthly_fraud_trend_2.png", plot = p3, width = 5, height = 5, dpi = 300)
```

### Moving Beyond Raw Observations: The Need for Statistical Rigor

While the observed fraud rates we've examined provide valuable initial insights, raw percentages can be misleading—especially when dealing with merchants or regions that have small transaction volumes. A merchant with 1 fraud case out of 5 transactions shows a 20% rate, but is this more concerning than a merchant with 50 fraud cases out of 1,000 transactions (5% rate)? 

We need a more sophisticated statistical approach to make reliable business decisions that account for sample size effects and provide confidence in our risk assessments.

### Conditional Probability & Bayesian Analysis Section
**What:**         Calculate observed conditional fraud rates—fraud cases as a proportion of total transactions—across key                    dimensions such as merchants and states, then apply Bayesian analysis to provide statistically-adjusted                    risk estimates.
**Why:**          Observed frequencies provide transparent baseline insights, while Bayesian methodology adds statistical                    rigor by accounting for sample size variations and incorporating prior knowledge about overall fraud                       patterns.
**Purpose:**      Present clear, data-driven insights into fraud risk concentration while demonstrating advanced                             statistical validation techniques that enhance the reliability of risk assessments.
**Limitations:**  This analysis uses simplified Bayesian methods with basic priors. Sample sizes near the minimum threshold                   (10 transactions) should be interpreted with appropriate statistical caution.

```{r observed-fraud-rates, fig.width=12, fig.height=6}
# Calculate overall fraud rate as prior knowledge
overall_fraud_rate <- mean(creditCardFraud$IsFraud)

cat("=== BAYESIAN STATISTICAL VALIDATION ===\n")
cat("Dataset overall fraud rate (prior):", round(100 * overall_fraud_rate, 2), "%\n\n")

# Function to compute Bayesian posterior with Laplace smoothing
compute_bayes_posterior <- function(fraud_cases, total_cases, prior, alpha = 1, beta = 1) {
  (fraud_cases + alpha * prior) / (total_cases + alpha + beta)
}

# State-level Bayesian fraud risk
state_bayes <- creditCardFraud %>%
  filter(!is.na(State), State != "") %>%
  group_by(State) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud),
    ObservedRate = 100 * FraudCases / TotalCases,
    BayesianRate = 100 * compute_bayes_posterior(FraudCases, TotalCases, overall_fraud_rate),
    .groups = "drop"
  ) %>%
  arrange(desc(BayesianRate))

cat("=== STATE-LEVEL BAYESIAN ANALYSIS ===\n")
cat("Top state (Bayesian-adjusted):", state_bayes$State[1], "\n")
cat("Observed rate:", round(state_bayes$ObservedRate[1], 1), "%\n")
cat("Bayesian-adjusted rate:", round(state_bayes$BayesianRate[1], 1), "%\n\n")

# Merchant-level Bayesian analysis for top Bayesian state
top_bayes_state <- state_bayes$State[1]

merchant_bayes <- creditCardFraud %>%
  filter(State == top_bayes_state) %>%
  filter(!is_invalid_name(MerchantName)) %>%
  group_by(MerchantName) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud),
    ObservedRate = 100 * FraudCases / TotalCases,
    BayesianRate = 100 * compute_bayes_posterior(FraudCases, TotalCases, overall_fraud_rate),
    .groups = "drop"
  ) %>%
  filter(TotalCases >= 10) %>%
  arrange(desc(BayesianRate)) %>%
  slice_head(n = 10) %>%
  mutate(MerchantName = factor(MerchantName, levels = rev(unique(MerchantName))))

# Convert factor back to character for display
merchant_bayes$MerchantName <- as.character(merchant_bayes$MerchantName)

cat("=== MERCHANT-LEVEL BAYESIAN ANALYSIS ===\n")
cat("Top merchant (Bayesian-adjusted):", merchant_bayes$MerchantName[1], "\n")
cat("Observed rate:", round(merchant_bayes$ObservedRate[1], 1), "%\n") 
cat("Bayesian-adjusted rate:", round(merchant_bayes$BayesianRate[1], 1), "%\n\n")

# Create Bayesian merchant visualization
p_merchant_bayes <- ggplot(merchant_bayes, aes(x = MerchantName, y = BayesianRate, fill = BayesianRate)) +
  geom_col() +
  scale_fill_gradient(low = "lightyellow", high = "red", name = "Bayesian Rate (%)") +
  geom_text(aes(label = sprintf("%.1f%%", BayesianRate)), hjust = -0.1, size = 4) +
  coord_flip(clip = "off") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.3))) +
  labs(
    title = paste("Top 10 Merchants by Bayesian Fraud Rate in", top_bayes_state),
    subtitle = "Statistically-adjusted estimates using Bayesian methodology",
    x = "Merchant",
    y = "Bayesian Fraud Rate (%)"
  ) +
  theme_light() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 11, face = "bold"),
    plot.margin = ggplot2::margin(t = 20, r = 120, b = 20, l = 20)
  )

print(p_merchant_bayes)

ggsave(paste0(output_dir, "/bayesian_top_merchants_", tolower(top_bayes_state), ".png"), 
       plot = p_merchant_bayes, width = 8, height = 5, dpi = 300)
```

### Applying Bayesian Statistical Methods

Now let's apply our refined statistical methodology to generate more reliable risk estimates. By incorporating prior knowledge about overall fraud patterns with observed data from each merchant and state, we can produce statistically-adjusted fraud risk scores that account for sample size variations and provide more confident business intelligence.

### Bayesian Fraud Risk Analysis
**What:**         Calculate posterior fraud probabilities for merchants and states using Bayes’ Theorem, incorporating observed data and                   priors for smoothing.

**Why:**          Bayesian analysis improves fraud risk estimates by combining prior beliefs with observed data, handling cases with                       small sample sizes better than raw frequencies.

**Purpose:**      To provide stakeholders with probabilistic fraud risk estimates, accounting for uncertainty and sample size effects.

**Limitations:**  Simplified Bayesian model without complex hierarchical or MCMC modeling; assumes prior fraud rate based on overall                       dataset.

How we do it:
Define an overall fraud prior rate (e.g., dataset-wide fraud rate). 
For each merchant (or state), calculate likelihoods (fraud and total transaction counts).
Compute posterior fraud probabilities with Laplace smoothing to avoid zero probabilities.
Display top merchants and states ranked by Bayesian fraud risk.

Statistical Reliability Caveat: Our minimum sample size filter (≥10 transactions) helps 
exclude the smallest merchants, but rates near this threshold should be interpreted 
cautiously. A 10% fraud rate based on 10 transactions carries much more uncertainty 
than a 1.5% rate based on 150 transactions. When prioritizing fraud prevention resources, 
consider both the statistical reliability of the rate estimate and the absolute volume 
of fraud cases to avoid overreacting to small-sample anomalies.

```{r}
### Bayesian Fraud Risk Analysis

# Overall fraud rate in dataset (prior)
overall_fraud_rate <- mean(creditCardFraud$IsFraud)

# Function to compute Bayesian posterior with Laplace smoothing
compute_bayes_posterior <- function(fraud_cases, total_cases, prior, alpha = 1, beta = 1) {
  (fraud_cases + alpha * prior) / (total_cases + alpha + beta)
}

# Merchant-level Bayesian fraud risk
merchant_bayes <- creditCardFraud %>%
  filter(State == top_state) %>%
  filter(
    !is.na(MerchantName), 
    str_trim(MerchantName) != "",
    !str_to_lower(str_trim(MerchantName)) %in% c("na", "n/a", "unknown", "#n/a")
  ) %>%
  group_by(MerchantName) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud)
  ) %>%
  filter(TotalCases >= 10) %>%
  mutate(
    BayesianFraudRate = compute_bayes_posterior(FraudCases, TotalCases, overall_fraud_rate)
  ) %>%
  arrange(desc(BayesianFraudRate)) %>%
  slice_head(n = 10) %>%
  mutate(MerchantName = factor(MerchantName, levels = rev(unique(MerchantName))))

# State-level Bayesian fraud risk
state_bayes <- creditCardFraud %>%
  filter(!is.na(State), State != "") %>%
  group_by(State) %>%
  summarise(
    TotalCases = n(),
    FraudCases = sum(IsFraud)
  ) %>%
  mutate(
    BayesianFraudRate = compute_bayes_posterior(FraudCases, TotalCases, overall_fraud_rate)
  ) %>%
  arrange(desc(BayesianFraudRate))

# Plot Bayesian fraud rates for top merchants
p_merchant_bayes <- ggplot(merchant_bayes, aes(x = MerchantName, y = BayesianFraudRate * 100, fill = BayesianFraudRate * 100)) +
  geom_col() +
  scale_fill_gradient(low = "lightyellow", high = "red", name = "Bayesian Rate (%)") +
  geom_text(
    aes(label = sprintf("%.1f%%", BayesianFraudRate * 100)),
    hjust = -0.1,
    size = 5
  ) +
  coord_flip(clip = "off") +
  scale_y_continuous(
    labels = scales::comma_format(accuracy = 1),
    expand = expansion(mult = c(0, 0.3))
  ) +
  labs(
    title = str_wrap("Top 10 Merchants by Bayesian Fraud Rate (%)", width = 30),
    x = "Merchant",
    y = "Bayesian Fraud Rate (%)"
  ) +
  theme_light() +
  theme(
  plot.title = element_text(size = 16, face = "bold"),
  axis.title = element_text(size = 12, face = "bold"),
  axis.text = element_text(size = 10),
  axis.text.x = element_text(angle = 45, hjust = 1),
  legend.text = element_text(size = 11),
  legend.title = element_text(size = 11, face = "bold"),
  plot.margin = ggplot2::margin(t = 20, r = 120, b = 20, l = 20),
  plot.title.position = "plot"
)

# Plot Bayesian fraud rates for states
p_state_bayes <- ggplot(state_bayes, aes(x = reorder(State, BayesianFraudRate), y = BayesianFraudRate * 100, fill = BayesianFraudRate * 100)) +
  geom_col() +
  scale_fill_gradient(low = "lightyellow", high = "red", name = "Bayesian Rate (%)") +
  geom_col() +
  coord_flip() +
  labs(
    title = "Bayesian Fraud Rate by State (%)",
    x = "State",
    y = "Bayesian Fraud Rate (%)"
  ) +
  theme_light() +
  theme(
  plot.title = element_text(size = 16, face = "bold"),
  axis.title = element_text(size = 12, face = "bold"),
  axis.text = element_text(size = 10),
  axis.text.x = element_text(angle = 45, hjust = 1),
  legend.text = element_text(size = 11),
  legend.title = element_text(size = 11, face = "bold"),
  plot.margin = ggplot2::margin(t = 20, r = 120, b = 20, l = 20),
  plot.title.position = "plot"
)

# Display merchant chart by itself

print(p_merchant_bayes)

# Save merchant chart
ggsave(
  filename = paste0(output_dir, "/bayesian_top_merchants_fixed.png"),
  plot = p_merchant_bayes,
  width = 8, height = 5,
  dpi = 300
)

# Display state chart by itself
print(p_state_bayes)

# Save state chart
ggsave(
  filename = paste0(output_dir, "/bayesian_fraud_rate_by_state.png"),
  plot = p_state_bayes,
  width = 6, height = 5,
  dpi = 300
)

```
### Strategic Synthesis: Translating Analysis into Action

Based on our comprehensive analysis across geographic, merchant, temporal, and statistical dimensions, several clear patterns emerge that inform evidence-based fraud prevention strategies.

#### Key Findings Summary

**Geographic Risk Assessment:** Our analysis identifies significant fraud rate variations across states, with the highest-risk state showing elevated fraud activity that warrants targeted intervention. This geographic concentration provides opportunities for focused resource allocation.

**Merchant-Level Risk Patterns:** Within high-risk regions, specific merchants demonstrate substantially elevated fraud rates. The combination of observed and Bayesian-adjusted rates confirms that these patterns represent genuine risk signals rather than random variation.

**Temporal Intelligence:** Multi-scale temporal analysis reveals both seasonal trends and short-term volatility patterns, enabling dynamic resource optimization throughout the year.

**Statistical Validation:** Bayesian methodology confirms that observed fraud patterns exceed what would be expected from random variation, providing confidence for business decision-making.

#### Strategic Recommendations

**Immediate Actions:**
- Priority geographic focus on the highest-risk state identified through analysis
- Enhanced monitoring protocols for merchants with statistically-confirmed elevated fraud rates
- Implementation of temporal monitoring adjustments based on observed seasonal patterns

**Implementation Framework:**
- Deploy risk scoring methodology incorporating both observed and Bayesian-adjusted metrics
- Establish dynamic resource allocation protocols based on evidence-derived risk profiles
- Create monitoring intensity schedules aligned with temporal fraud patterns

**Success Metrics:**
- Fraud rate reduction in targeted geographic regions
- Improved detection accuracy for high-risk merchant categories
- Optimized resource utilization based on temporal patterns
- Overall fraud prevention effectiveness improvements
---

_Source Info:_  
This analysis is based on data from 2023–2024, generated on `r format(Sys.time(), "%B %d, %Y at %H:%M:%S")`. All steps and code are fully documented and reproducible.
